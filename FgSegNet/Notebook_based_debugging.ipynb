{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Sep 29 22:57:12 2017\n",
    "\n",
    "@author: longang\n",
    "@comments by Zhi Zeng\n",
    "\"\"\"\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Activation, Input, Dropout\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose, UpSampling2D, Cropping2D, MyUpSampling2D\n",
    "\"\"\"\n",
    "Note: The module MyUpSampling2D is written by the author.\n",
    "\"\"\"\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras import regularizers\n",
    "\n",
    "class FgSegNetModule(object):\n",
    "    \n",
    "    def __init__(self, lr, reg, img_shape, scene, vgg_weights_path):\n",
    "        self.lr = lr # Is this the learning rate ??\n",
    "        self.reg = reg # The weight for the l2 regularizer for the Tconv\n",
    "        self.img_shape = img_shape # the shape of the image ??\n",
    "        self.scene = scene # ???\n",
    "        self.vgg_weights_path = vgg_weights_path # the path for the vgg_weights\n",
    "\n",
    "    def VGG16(self, x): \n",
    "        \"\"\"\n",
    "        Note: This defines the structure of the vgg16. Note that each layer has a unique name in the definition.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Block 1\n",
    "        x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1', data_format='channels_last')(x)\n",
    "        x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "        x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "        \"\"\"\n",
    "        Note:\n",
    "        data_format: A string, one of channels_last (default) or channels_first. \n",
    "        The ordering of the dimensions in the inputs. \n",
    "        channels_last corresponds to inputs with shape (batch, height, width, channels) \n",
    "        while channels_first corresponds to inputs with shape (batch, channels, height, width). \n",
    "        It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json. \n",
    "        If you never set it, then it will be \"channels_last\".\n",
    "        \"\"\"\n",
    "        # Block 2\n",
    "        x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "        x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "        x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "    \n",
    "        # Block 3\n",
    "        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "    \n",
    "        # Block 4\n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "        x = Dropout(0.5, name='dr1')(x)\n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "        x = Dropout(0.5, name='dr2')(x)\n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "        x = Dropout(0.5, name='dr3')(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def transposedConv(self, x):\n",
    "        \"\"\"\n",
    "        Note:\n",
    "        Regularizers allow to apply penalties on layer parameters or layer activity during optimization. \n",
    "        These penalties are incorporated in the loss function that the network optimizes.\n",
    "        The penalties are applied on a per-layer basis. \n",
    "        The exact API will depend on the layer, but the layers Dense, Conv1D, Conv2D and Conv3D have a unified API.\n",
    "        These layers expose 3 keyword arguments:\n",
    "        kernel_regularizer: instance of keras.regularizers.Regularizer\n",
    "        bias_regularizer: instance of keras.regularizers.Regularizer\n",
    "        activity_regularizer: instance of keras.regularizers.Regularizer\n",
    "        Example\n",
    "        from keras import regularizers\n",
    "        model.add(Dense(64, input_dim=64,\n",
    "                        kernel_regularizer=regularizers.l2(0.01),\n",
    "                        activity_regularizer=regularizers.l1(0.01)))\n",
    "        Available penalties\n",
    "        keras.regularizers.l1(0.)\n",
    "        keras.regularizers.l2(0.)\n",
    "        keras.regularizers.l1_l2(0.)\n",
    "        For each of the Tconv block, there is a regularizer. \n",
    "        I am not sure why the regularizer should be used here?\n",
    "        \"\"\"\n",
    "        \n",
    "        # block 5\n",
    "        x = Conv2DTranspose(64, (1, 1), activation='relu', padding='same', name='block5_tconv1', \n",
    "                            kernel_regularizer=regularizers.l2(self.reg))(x)\n",
    "        x = Conv2DTranspose(64, (3, 3), activation='relu', padding='same', name='block5_tconv2')(x)\n",
    "        x = Conv2DTranspose(512, (1, 1), activation='relu', padding='same', name='block5_tconv3')(x)\n",
    "        \n",
    "        # block 6\n",
    "        x = Conv2DTranspose(64, (1, 1), activation='relu', padding='same', name='block6_tconv1', \n",
    "                            kernel_regularizer=regularizers.l2(self.reg))(x)\n",
    "        x = Conv2DTranspose(64, (5, 5), strides=(2, 2), activation='relu', padding='same', name='block6_tconv2')(x)\n",
    "        x = Conv2DTranspose(256, (1, 1), activation='relu', padding='same', name='block6_tconv3')(x)\n",
    "        \n",
    "        # block 7\n",
    "        x = Conv2DTranspose(64, (1, 1), activation='relu', padding='same', name='block7_tconv1', \n",
    "                            kernel_regularizer=regularizers.l2(self.reg))(x)\n",
    "        x = Conv2DTranspose(64, (3, 3), activation='relu', padding='same', name='block7_tconv2')(x)\n",
    "        x = Conv2DTranspose(128, (1, 1), activation='relu', padding='same', name='block7_tconv3')(x)\n",
    "        \n",
    "        # block 8\n",
    "        x = Conv2DTranspose(64, (5, 5), strides=(2, 2), activation='relu', padding='same', name='block8_conv1', \n",
    "                            kernel_regularizer=regularizers.l2(self.reg))(x)\n",
    "        \n",
    "        # block 9\n",
    "        x = Conv2DTranspose(1, (1, 1), padding='same', name='block9_conv1')(x)\n",
    "        x = Activation('sigmoid')(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def initModel (self):\n",
    "        h, w, d = self.img_shape # height width dimension\n",
    "        \n",
    "        input_1 = Input(shape=(h, w, d), name='ip_scale1')\n",
    "        vgg_layer_output = self.VGG16(input_1)\n",
    "        shared_model = Model(inputs=input_1, outputs=vgg_layer_output, name='shared_model')\n",
    "        shared_model.load_weights(self.vgg_weights_path, by_name=True)\n",
    "        \"\"\"\n",
    "        Note:\n",
    "        model.load_weights(filepath, by_name=False): \n",
    "        loads the weights of the model from a HDF5 file (created by save_weights). \n",
    "        By default, the architecture is expected to be unchanged. \n",
    "        To load weights into a different architecture (with some layers in common), \n",
    "        use by_name=True to load only those layers with the same name.\n",
    "        \"\"\"\n",
    "        \n",
    "        unfreeze_layers = ['block4_conv1','block4_conv2', 'block4_conv3']\n",
    "        for layer in shared_model.layers:\n",
    "            if(layer.name not in unfreeze_layers):\n",
    "                layer.trainable = False\n",
    "        \"\"\"\n",
    "        Note:\n",
    "        Only the last block of the encoder will be trained.\n",
    "        \"\"\"\n",
    "                \n",
    "        # Scale 1\n",
    "        x1_ups = {'streetCornerAtNight':(0,1), 'tramStation':(1,0), 'turbulence2':(1,0)} # ???\n",
    "        x1 = shared_model.output\n",
    "        \"\"\"\n",
    "        Cropping2D   \n",
    "        keras.layers.Cropping2D(cropping=((0, 0), (0, 0)), data_format=None)\n",
    "        Cropping layer for 2D input (e.g. picture).\n",
    "        It crops along spatial dimensions, i.e. width and height.\n",
    "        Arguments\n",
    "        cropping: int, or tuple of 2 ints, or tuple of 2 tuples of 2 ints.\n",
    "        If int: the same symmetric cropping is applied to width and height.\n",
    "        If tuple of 2 ints: interpreted as two different symmetric cropping values for height and width: \n",
    "            (symmetric_height_crop, symmetric_width_crop).\n",
    "        If tuple of 2 tuples of 2 ints: interpreted as ((top_crop, bottom_crop), (left_crop, right_crop))\n",
    "        data_format: A string, one of channels_last (default) or channels_first. \n",
    "        Examples\n",
    "        # Crop the input 2D images or feature maps\n",
    "        model = Sequential()\n",
    "        model.add(Cropping2D(cropping=((2, 2), (4, 4)),\n",
    "                             input_shape=(28, 28, 3)))\n",
    "        # now model.output_shape == (None, 24, 20, 3)\n",
    "        model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "        model.add(Cropping2D(cropping=((2, 2), (2, 2))))\n",
    "        # now model.output_shape == (None, 20, 16. 64)\n",
    "        \"\"\"\n",
    "        if(self.scene=='wetSnow'):\n",
    "            x1 = Cropping2D(cropping=((1, 2),(0, 0)))(x1)\n",
    "        elif(self.scene=='skating'):\n",
    "            x1 = Cropping2D(cropping=((1, 1),(1, 2)))(x1)\n",
    "        else:\n",
    "            for key, val in x1_ups.items():\n",
    "                if self.scene==key:\n",
    "                    # upscale by adding number of pixels to each dim.\n",
    "                    x1 = MyUpSampling2D(size=(1,1), num_pixels=val)(x1)\n",
    "                    break\n",
    "        \"\"\"\n",
    "        Note:\n",
    "        For the first scale, only expending operations are used.\n",
    "        \"\"\"\n",
    "            \n",
    "        # Scale 2\n",
    "        x2_ups = {'tunnelExit_0_35fps':(0,1),'tramCrossroad_1fps':(1,0),'bridgeEntry':(1,1),\n",
    "                  'busyBoulvard':(1,0),'fluidHighway':(0,1),'streetCornerAtNight':(1,1), \n",
    "                  'tramStation':(2,0),'winterStreet':(1,0),'twoPositionPTZCam':(1,0),\n",
    "                  'peopleInShade':(1,1),'turbulence2':(1,1),'turbulence3':(1,0),\n",
    "                  'skating':(1,1), 'wetSnow':(0,0)}\n",
    "        \n",
    "        input_2 = Input(shape=(int(h/2), int(w/2), d), name='ip_scale2')\n",
    "        x2 = shared_model(input_2)\n",
    "        x2 = UpSampling2D((2,2))(x2)\n",
    "        for key, val in x2_ups.items():\n",
    "            if self.scene == key and self.scene in ['skating', 'wetSnow']:\n",
    "                x2 = Cropping2D(cropping=((1, 1), val))(x2)\n",
    "                break\n",
    "            elif self.scene==key:\n",
    "                x2 = MyUpSampling2D(size=(1, 1), num_pixels=val)(x2)\n",
    "                break\n",
    "        \"\"\"\n",
    "        Note:\n",
    "        Multiple input fashion in Keras. Nothing but updating and expending.\n",
    "        \"\"\"\n",
    "            \n",
    "        # Scale 3\n",
    "        x3_ups = {'tunnelExit_0_35fps':(2,3),'tramCrossroad_1fps':(3,0),'bridgeEntry':(3,1,),\n",
    "                  'busyBoulvard':(3,0),'fluidHighway':(0,3),'streetCornerAtNight':(1,1),\n",
    "                  'tramStation':(2,0),'winterStreet':(1,0),'twoPositionPTZCam':(1,2),\n",
    "                  'peopleInShade':(1,3),'turbulence2':(3,1),'turbulence3':(1,0),\n",
    "                  'office':(0,2), 'pedestrians':(0,2), 'bungalows':(0,2), 'busStation':(0,2)}\n",
    "                \n",
    "        input_3 = Input(shape=(int(h/4), int(w/4), d), name='ip_scale3')\n",
    "        x3 = shared_model(input_3)\n",
    "        x3 = UpSampling2D((4,4))(x3)\n",
    "        for key, val in x3_ups.items():\n",
    "            if self.scene==key:\n",
    "                x3 = MyUpSampling2D(size=(1,1), num_pixels=val)(x3)\n",
    "                break\n",
    "        \"\"\"\n",
    "        Note:\n",
    "        Nothing but upsampling and expending.\n",
    "        \"\"\"\n",
    "            \n",
    "        # concatenate feature maps\n",
    "        top = keras.layers.concatenate([x1, x2, x3], name='feature_concat')\n",
    "        if(self.scene=='wetSnow'):\n",
    "            top = MyUpSampling2D(size=(1,1), num_pixels=(3,0))(top)\n",
    "        elif(self.scene=='skating'):\n",
    "            top = MyUpSampling2D(size=(1,1), num_pixels=(2,3))(top)\n",
    "        \"\"\"\n",
    "        Note:\n",
    "        Concatenate and expending\n",
    "        \"\"\"\n",
    "        \n",
    "        # Transposed Conv\n",
    "        top = self.transposedConv(top)\n",
    "        # i chose this crazy upscaling/cropping way\n",
    "        if(self.scene=='tramCrossroad_1fps'):\n",
    "            top = MyUpSampling2D(size=(1,1), num_pixels=(2,0))(top)\n",
    "        elif(self.scene=='bridgeEntry'):\n",
    "            top = MyUpSampling2D(size=(1,1), num_pixels=(2,2))(top)\n",
    "        elif(self.scene=='fluidHighway'):\n",
    "            top = MyUpSampling2D(size=(1,1), num_pixels=(2,0))(top)\n",
    "        elif(self.scene=='streetCornerAtNight'): \n",
    "            top = MyUpSampling2D(size=(1,1), num_pixels=(1,0))(top)\n",
    "            top = Cropping2D(cropping=((0, 0),(0, 1)))(top)\n",
    "        elif(self.scene=='tramStation'):  \n",
    "            top = Cropping2D(cropping=((1, 0),(0, 0)))(top)\n",
    "        elif(self.scene=='twoPositionPTZCam'):\n",
    "            top = MyUpSampling2D(size=(1,1), num_pixels=(0,2))(top)\n",
    "        elif(self.scene=='turbulence2'):\n",
    "            top = Cropping2D(cropping=((1, 0),(0, 0)))(top)\n",
    "            top = MyUpSampling2D(size=(1,1), num_pixels=(0,1))(top)\n",
    "        elif(self.scene=='turbulence3'):\n",
    "            top = MyUpSampling2D(size=(1,1), num_pixels=(2,0))(top)\n",
    "\n",
    "        vision_model = Model(inputs=[input_1, input_2, input_3], outputs=top, name='vision_model')\n",
    "        opt = keras.optimizers.RMSprop(lr = self.lr, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "        vision_model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "        return vision_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Sep 29 22:22:22 2017\n",
    "\n",
    "@author: longang\n",
    "@Comments by Zhi Zeng\n",
    "\"\"\"\n",
    "\n",
    "get_ipython().magic(u'load_ext autoreload')\n",
    "get_ipython().magic(u'autoreload 2')\n",
    "\n",
    "\"\"\"\n",
    "Note:\n",
    "autoreload ensures that we can run a imported function, then, change the function in an editor and the changed function can be reloaded\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Note:\n",
    "get_ipython().magic('zzz') equals to %zzz\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import os\n",
    "\n",
    "# =============================================================================\n",
    "#  For reprodocable results\n",
    "# =============================================================================\n",
    "#os.environ['PYTHONHASHSEED'] = '0'\n",
    "# Environment variables are accessed through os.environ\n",
    "# If PYTHONHASHSEED is set to an integer value, it is used as a fixed seed for generating the hash() of the types covered by the hash randomization.\n",
    "#np.random.seed(42)\n",
    "#rn.seed(12345)\n",
    "#session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "# ConfigProto is used to configurate sesson()\n",
    "# For both configuration options, if they are unset or set to 0, will default to the number of logical CPU cores. Testing has shown that the default is effective for systems ranging from one CPU with 4 cores to multiple CPUs with 70+ combined logical cores. A common alternative optimization is to set the number of threads in both pools equal to the number of physical cores rather than logical cores.\n",
    "from keras import backend as K\n",
    "#tf.set_random_seed(1234)\n",
    "#sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "#K.set_session(sess)\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "K.set_session(sess)\n",
    "\n",
    "import keras, glob\n",
    "from keras.preprocessing import image as kImage\n",
    "from skimage.transform import pyramid_gaussian\n",
    "from sklearn.utils import compute_class_weight\n",
    "# from FgSegNetModule import FgSegNetModule\n",
    "from keras.utils.data_utils import get_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Few frames, load into memory directly\n",
    "# =============================================================================\n",
    "def generateData(train_dir, dataset_dir, scene):\n",
    "    \n",
    "    # It is weired that the parameter 'scene'is not used in the function\n",
    "    \n",
    "    void_label = -1. # The masked regions will be labeled as -1\n",
    "    X_list = []\n",
    "    Y_list = []\n",
    "    \n",
    "    # Given ground-truths, load training frames\n",
    "    # ground-truths end with '*.png'\n",
    "    # training frames end with '*.jpg'\n",
    "    \n",
    "    # scan over FgSegNet_dataset for groundtruths\n",
    "    for root, _, _ in os.walk(train_dir):\n",
    "        # Walk though the tree (root, dirs, files)\n",
    "        gtlist = glob.glob(os.path.join(root,'*.png'))\n",
    "        # list all *.png files\n",
    "        if gtlist:\n",
    "            Y_list = gtlist\n",
    "        # I think the following two lines should be add\n",
    "        else:\n",
    "            Y_list += gtlist\n",
    "    \n",
    "    # scan over CDnet2014_dataset for .jpg files\n",
    "    for root, _, _ in os.walk(dataset_dir):\n",
    "        inlist = glob.glob(os.path.join(root,'*.jpg'))\n",
    "        if inlist:\n",
    "            X_list = inlist\n",
    "        # I think the following two lines should be add\n",
    "        else:\n",
    "            X_list += inlist\n",
    "    \n",
    "    # filter matched files        \n",
    "    # brutal force match\n",
    "    X_list_temp = []\n",
    "    for i in range(len(Y_list)):\n",
    "        Y_name = os.path.basename(Y_list[i])\n",
    "        Y_name = Y_name.split('.')[0]\n",
    "        Y_name = Y_name.split('gt')[1]\n",
    "        for j in range(len(X_list)):\n",
    "            X_name = os.path.basename(X_list[j])\n",
    "            X_name = X_name.split('.')[0]\n",
    "            X_name = X_name.split('in')[1]\n",
    "            if (Y_name == X_name):\n",
    "                X_list_temp.append(X_list[j])\n",
    "                break\n",
    "    X_list = X_list_temp\n",
    "    # The following line is negligable\n",
    "    # del X_list_temp, inlist, gtlist\n",
    "    \n",
    "    # process training images\n",
    "    X = []\n",
    "    Y = []\n",
    "    # Load all images and modify the lables\n",
    "    for i in range(0, len(X_list)):\n",
    "        x = kImage.load_img(X_list[i])\n",
    "        x = kImage.img_to_array(x)\n",
    "        X.append(x)\n",
    "        # what is the type of X ?\n",
    "        # Is the range of an element in X [0, 255] or [0,1]?\n",
    "        \n",
    "        x = kImage.load_img(Y_list[i], grayscale = True)\n",
    "        x = kImage.img_to_array(x)\n",
    "        shape = x.shape\n",
    "        x /= 255.0\n",
    "        x = x.reshape(-1)\n",
    "        idx = np.where(np.logical_and(x>0.25, x<0.8))[0] # find non-ROI\n",
    "        if (len(idx)>0):\n",
    "            x[idx] = void_label # void_label = -1\n",
    "            # After this operation, elements in Y can be only one of [0.0, 1.0, -1.0]\n",
    "        x = x.reshape(shape)\n",
    "        x = np.floor(x)\n",
    "        Y.append(x)\n",
    "    # The following line is negligable\n",
    "    # del Y_list, X_list, x, idx\n",
    "    X = np.asarray(X) # convert X to np.array\n",
    "    Y = np.asarray(Y)\n",
    "    \n",
    "    # Shuffle the training data\n",
    "    idx = list(range(X.shape[0])) # np.arange(X.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    np.random.shuffle(idx)\n",
    "    X = X[idx]\n",
    "    Y = Y[idx]\n",
    "    # The following line is negligable\n",
    "    # del idx\n",
    "    \n",
    "    # Image Pyramid\n",
    "    scale1 = X\n",
    "    # del X\n",
    "    scale2 = []\n",
    "    scale3 = []\n",
    "    for i in range(0, scale1.shape[0]):\n",
    "        pyramid = tuple(pyramid_gaussian(scale1[i]/255., max_layer=2, downscale=2))\n",
    "        scale2.append(pyramid[1]) # 2nd scale\n",
    "        scale3.append(pyramid[2]) # 3rd scale\n",
    "        # del pyramid\n",
    "    scale2 = np.asarray(scale2)\n",
    "    scale3 = np.asarray(scale3)\n",
    "    # I think the following line should added:\n",
    "    scale1 = X/255.\n",
    "    print (scale1.shape, scale2.shape, scale3.shape)\n",
    "\n",
    "    # compute class weights\n",
    "    cls_weight_list = []\n",
    "    for i in range(Y.shape[0]):\n",
    "        # For each groundtruth image, the auther compute the class weights. These class weights are stored in a list\n",
    "        # I do not know why the author need this.\n",
    "        y = Y[i].reshape(-1) # flattern Y\n",
    "        idx = np.where(y!=void_label)[0] # for unmasked pixels\n",
    "        if(len(idx)>0):\n",
    "            y = y[idx] # exclude masked pixels\n",
    "        lb = np.unique(y) #  0., 1.0\n",
    "        cls_weight = compute_class_weight('balanced', lb , y)\n",
    "        # cls_weight = n_samples/(n_classes*np.bincount(y))\n",
    "        # np.bincount count the num of occurence of each non-neg ints\n",
    "        class_0 = cls_weight[0]\n",
    "        class_1 = cls_weight[1] if len(lb)>1 else 1.0\n",
    "        \n",
    "        cls_weight_dict = {0:class_0, 1: class_1}\n",
    "        cls_weight_list.append(cls_weight_dict)\n",
    "    # del y, idx\n",
    "    cls_weight_list = np.asarray(cls_weight_list)\n",
    "    # Three kinds of inputs, one outputs, and one weight list\n",
    "    return [scale1, scale2, scale3, Y, cls_weight_list]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
